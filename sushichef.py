#!/usr/bin/env python
from bs4 import BeautifulSoup
import codecs
from collections import defaultdict, OrderedDict
import copy
from git import Repo
import glob
from le_utils.constants import licenses, content_kinds, file_formats
import hashlib
import json
import logging
import markdown2
import ntpath
import os
from pathlib import Path
import re
import requests
from ricecooker.classes.licenses import get_license
from ricecooker.chefs import JsonTreeChef
from ricecooker.utils import downloader, html_writer
from ricecooker.utils.caching import (
    CacheForeverHeuristic,
    FileCache,
    CacheControlAdapter,
)
from ricecooker.utils.html import download_file
from ricecooker.utils.jsontrees import write_tree_to_json_tree, SUBTITLES_FILE
from ricecooker.utils.zip import create_predictable_zip

import tempfile
import time
from urllib.error import URLError
from urllib.parse import urljoin
from urllib.parse import urlparse, parse_qs
from utils import dir_exists, get_name_from_url, clone_repo, build_path
from utils import file_exists, get_video_resolution_format, remove_links
from utils import get_name_from_url_no_ext, get_node_from_channel, get_level_map
from utils import remove_iframes, get_confirm_token, save_response_content
from utils import link_to_text, remove_scripts
import youtube_dl
from urllib.parse import urlparse
import sys
import xxhash

sys.setrecursionlimit(1200)


requests.packages.urllib3.disable_warnings()
requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ":HIGH:!DH:!aNULL"
try:
    requests.packages.urllib3.contrib.pyopenssl.util.ssl_.DEFAULT_CIPHERS += (
        ":HIGH:!DH:!aNULL"
    )
except AttributeError:
    # no pyopenssl support used / needed / available
    pass


DATA_DIR = "chefdata"
DATA_DIR_SUBJECT = ""
COPYRIGHT_HOLDER = "CSU and Merlot"
LICENSE = get_license(licenses.CC_BY_NC_SA, copyright_holder=COPYRIGHT_HOLDER).as_dict()
AUTHOR = "CSU and Merlot"

LOGGER = logging.getLogger()
__logging_handler = logging.StreamHandler()
LOGGER.addHandler(__logging_handler)
LOGGER.setLevel(logging.INFO)

DOWNLOAD_VIDEOS = True
DOWNLOAD_FILES = True
OVERWRITE = True

sess = requests.Session()
cache = FileCache(".webcache")
basic_adapter = CacheControlAdapter(cache=cache)
forever_adapter = CacheControlAdapter(heuristic=CacheForeverHeuristic(), cache=cache)


"""
This is the jerarchery in libretext.

- Collection (CourseLibreText, TextBook, Homework)
  - CategoryA
     * CategoryB (optional)
       * Chapter (optional)
       - Index
         - Chapter

- Collection (Reference, Demos)
     - Index
       - Chapter

- Collection (VisualizationA)
  - CategoryA
     - CategoryB
        - Visualization

- Collection (Ancillary Materials)
  - CategoryA
     * Category B
     - Index
       - Visualization
"""


SUBJECTS = {
    "phys": "https://phys.libretexts.org/",
    "chem": "https://chem.libretexts.org/",
    "bio": "https://bio.libretexts.org/",
    "eng": "https://eng.libretexts.org/",
    "math": "https://math.libretexts.org/",
}

CHANNEL_NAMES = {
    "phys": "LibreTexts Physics",
    "chem": "LibreTexts Chemistry",
    "bio": "LibreTexts Biology",
    "eng": "LibreTexts Engineering",
    "math": "LibreTexts Mathematics",
}


SUBJECTS_THUMBS = {
    "phys": "https://phys.libretexts.org/@api/deki/files/3289/libretexts_section_complete_phys350.png",
    "chem": "https://chem.libretexts.org/@api/deki/files/85425/libretexts_section_complete_chem_sm_124.png",
    "bio": "https://bio.libretexts.org/@api/deki/files/8208/libretexts_section_complete_bio_header.png",
    "eng": "https://eng.libretexts.org/@api/deki/files/1442/libretexts_section_complete_engineering_325.png",
    "math": "https://math.libretexts.org/@api/deki/files/1742/libretexts_section_complete_math350_sigma.png",
}


def hashed(string_to_hash):
    return xxhash.xxh64(string_to_hash.encode("utf-8")).hexdigest()


class Browser:
    def __init__(self, url):
        self.url = url

    def run(self, from_i=1, to_i=None):
        document = download(self.url)
        if document is not None:
            soup = BeautifulSoup(document, "html5lib")  # html.parser
        section = soup.find("section", class_="mt-content-container")
        section_div = section.find("div", class_="noindex")
        for tag_a in section_div.find_all("a"):
            yield tag_a


class LinkCollection:
    def __init__(self, links):
        self.links = links
        self.to_collection()

    def to_collection(self):
        self.collection = []
        nb_link = 0
        for link in self.links:
            if nb_link > 2:
                break
            nb_link += 1
            self.collection.append(Collection(link.text, link.attrs.get("href", "")))

    def to_node(self):
        nb_col = 0
        for collection in self.collection:
            if nb_col > 2:
                break
            nb_col += 1
            yield collection.to_node()


class Collection:
    url_names = [
        "Courses",
        "Bookshelves",
        "Homework_Exercises",
        "Ancillary_Materials",
        "Visualizations_and_Simulations",
    ]

    def __init__(self, title, link):
        self.title = title
        self.source_id = link
        self.collection = {
            CourseLibreTexts.title: CourseLibreTexts,
            TextBooksTextMaps.title: TextBooksTextMaps,
            HomeworkExercices.title: HomeworkExercices,
            Homework.title: Homework,
            VisualizationPhEt.title: VisualizationPhEt,
            VisualizationsSimulations.title: VisualizationsSimulations,
        }
        self.nb_topic = 2

    def to_node(self):
        try:
            Topic = self.collection[self.title]
        except KeyError:
            LOGGER.error("Collection Not Found: {}".format(self.title))
        else:
            LOGGER.info(self.title)
            topic = Topic(self.source_id)
            topic.populate_thumbnails()
            topic.units()
            return topic.to_node()


class Topic(object):
    def __init__(self, url, title=None):
        if title is not None:
            self.title = title
        self.source_id = url
        self.urls = Browser(self.source_id).run()
        self.lang = "en"
        self.tree_nodes = OrderedDict()
        self.thumbnails_links = {}
        self.description = ""
        self.soup = self.to_soup()
        LOGGER.info("- " + self.title)

    def to_soup(self):
        document = download(self.source_id)
        if document is not None:
            return BeautifulSoup(document, "html5lib")  # html5lib

    def __iter__(self):
        return self.urls

    def __next__(self):
        return next(self.urls)

    def populate_thumbnails(self):
        pass

    def add_node(self, node):
        if node is not None:
            self.tree_nodes[node["source_id"]] = node

    def to_node(self):
        return dict(
            kind=content_kinds.TOPIC,
            source_id=self.title,
            title=self.title,
            description=self.description,
            language=self.lang,
            author="",
            license=LICENSE,
            children=list(self.tree_nodes.values()),
        )


class CourseLibreTexts(Topic):
    title = "Course Shells"  # previously "Campus Courses", "Course LibreTexts"

    def units(self):
        nb_url = 0
        for url in self:
            if nb_url > 2:
                break
            nb_url += 1
            topic = Topic(url.attrs.get("href"), title=url.text)
            for link in topic:
                print("CourseLibreTexts", link)
                course_index = CourseIndex(link.text, link.attrs.get("href"))
                course_index.description = link.attrs.get("title")
                path = [
                    DATA_DIR,
                    DATA_DIR_SUBJECT,
                    hashed(topic.title),
                    hashed(link.text),
                ]
                course_index.index(build_path(path))
                topic.add_node(course_index.to_node())
            self.add_node(topic.to_node())


class TextBooksTextMaps(Topic):
    title = "Bookshelves"  # "TextBooks & TextMaps"

    def populate_thumbnails(self):
        self.thumbnails_links = thumbnails_links(self.soup, "li", "mt-sortable-listing")

    def units(self):
        base_path = [DATA_DIR, DATA_DIR_SUBJECT, hashed(self.title)]
        nb_chapter = 0
        for chapter_link in self:
            if nb_chapter > 2:
                break
            nb_chapter += 1
            course_index = CourseIndex(
                chapter_link.text, chapter_link.attrs.get("href", "")
            )
            course_index.description = chapter_link.attrs.get("title")
            course_index.thumbnail = self.thumbnails_links.get(
                chapter_link.attrs.get("href", ""), None
            )
            course_index.index(build_path(base_path + [hashed(chapter_link.text)]))
            self.add_node(course_index.to_node())


class HomeworkExercices(Topic):
    title = "Homework Exercises"

    def populate_thumbnails(self):
        self.thumbnails_links = thumbnails_links(self.soup, "li", "mt-sortable-listing")

    def units(self):
        base_path = [DATA_DIR, DATA_DIR_SUBJECT, hashed(self.title)]
        nb_chapter = 0
        for chapter_link in self:
            if nb_chapter > 2:
                break
            nb_chapter += 1
            course_index = CourseIndex(
                chapter_link.text, chapter_link.attrs.get("href", "")
            )
            course_index.description = chapter_link.attrs.get("title")
            course_index.thumbnail = self.thumbnails_links.get(
                chapter_link.attrs.get("href", ""), None
            )
            course_index.index(build_path(base_path + [hashed(chapter_link.text)]))
            self.add_node(course_index.to_node())


class Homework(HomeworkExercices):  # Alias for homework and exercices
    title = "Homework"


class VisualizationPhEt(Topic):
    title = "Ancillary Materials"

    def units(self):
        base_path = [DATA_DIR, DATA_DIR_SUBJECT, hashed(self.title)]
        nb_chapter = 0
        for chapter_link in self:
            if nb_chapter > 2:
                break
            nb_chapter += 1
            if chapter_link.text.strip() in [
                "CalcPlot3D Interactive Figures",
                "GeoGebra Simulations",
            ]:
                continue
            course_index = CourseIndex(
                chapter_link.text, chapter_link.attrs.get("href", "")
            )
            course_index.description = chapter_link.attrs.get("title")
            course_index.thumbnail = self.thumbnails_links.get(
                chapter_link.attrs.get("href", ""), None
            )
            course_index.index(build_path(base_path + [hashed(chapter_link.text)]))
            self.add_node(course_index.to_node())


class VisualizationsSimulations(VisualizationPhEt):
    title = "Visualizations and Simulations"


def thumbnails_links(soup, tag, class_):
    if soup is not None:
        courses_list = soup.find_all(tag, class_=class_)
        thumnails = {}
        for course_li in courses_list:
            link = course_li.find("a").get("href")
            img = course_li.find("img")
            if img is not None:
                thumnails[link] = img["src"]
        return thumnails


def save_thumbnail(url, title):
    import imghdr
    from io import BytesIO

    try:
        r = requests.get(url)
    except:
        return None
    else:
        img_buffer = BytesIO(r.content)
        img_ext = imghdr.what(img_buffer)
        if img_ext in ["jpeg", "png"]:
            filename = "{}.{}".format(title, img_ext)
            base_dir = build_path([DATA_DIR, DATA_DIR_SUBJECT, "thumbnails"])
            filepath = os.path.join(base_dir, filename)
            with open(filepath, "wb") as f:
                f.write(img_buffer.read())
            return filepath


class CourseIndex(object):
    def __init__(self, title, url, visited_urls=None):
        self.source_id = url
        self.title = title
        self.lang = "en"
        self.description = None
        self.tree_nodes = OrderedDict()
        self.soup = self.to_soup()
        self.author()
        self._thumbnail = None
        self.visited_urls = visited_urls if visited_urls is not None else set([])
        LOGGER.info("----- Course Index title: " + self.title)
        LOGGER.info("-----    url: " + self.source_id)

    def to_soup(self, loadjs=False):
        document = download(self.source_id, loadjs=loadjs)
        try:
            response = requests.get(self.source_id, timeout=20)
        except Exception as exc:
            LOGGER.error(exc)
        else:
            if response.status_code == 200:
                self.source_id = response.url
            if document is not None:
                return BeautifulSoup(document, "html5lib")  # html5lib

    @property
    def thumbnail(self):
        return self._thumbnail

    @thumbnail.setter
    def thumbnail(self, url):
        self._thumbnail = save_thumbnail(url, self.title)

    def author(self):
        if self.soup is not None:
            div = self.soup.find("div", "mt-author-container")
            if div is not None:
                tag_a = div.find(
                    lambda tag: tag.name == "a"
                    and tag.findParent("li", class_="mt-author-information")
                )
                if tag_a is not None:
                    return tag_a.text

    def index(self, base_path):
        base_url_path_elems = urlparse(self.source_id).path.split("/")
        base_url_classes = Collection.url_names
        if (
            len(base_url_path_elems) == 2
            and base_url_path_elems[1] in base_url_classes
            or len(base_url_path_elems) == 1
        ):
            return "cycle"
        if self.soup is None:
            retry_times = 0
            while retry_times < 5 and self.soup is None:
                self.soup = self.to_soup()
                LOGGER.info("Retrying")
                retry_times += 1
            if self.soup is None:
                LOGGER.error("Could not download content ")
                return
        courses_link = self.soup.find_all(
            lambda tag: tag.name == "a"
            and tag.findParent("dt", class_="mt-listing-detailed-title")
        )
        if len(courses_link) == 0:
            courses_link = self.soup.find_all(
                lambda tag: tag.name == "a"
                and tag.findParent("li", class_="mt-sortable-listing")
            )
        if len(courses_link) == 0:
            query = QueryPage(self.soup, self.source_id)
            body = query.body()
            if body is not None:
                courses_link = body.find_all("a")
            else:
                courses_link = self.soup.find_all(
                    lambda tag: tag.name == "a"
                    and tag.findParent("div", class_="wiki-tree")
                )
                if len(courses_link) == 0:
                    return
                else:
                    LOGGER.info("OK")

        thumbnails = thumbnails_links(self.soup, "li", "mt-sortable-listing")

        index_base_path = base_path  # build_path([base_path])
        nb_items = 0
        for course_link in courses_link:
            course_link_href = course_link.attrs.get("href", "")
            if course_link_href in self.visited_urls:
                continue
            if nb_items > 2:
                break
            print("MAIN COURSE LINK", nb_items, course_link_href)
            nb_items += 1
            self.visited_urls.add(course_link_href)
            document = download(course_link_href)
            chapter_basepath = build_path([index_base_path, hashed(course_link.text)])
            if document is not None:
                query = QueryPage(
                    BeautifulSoup(document, "html.parser"), course_link_href
                )

                course_body = query.body()
                nb_chapter = 0
                if course_body is not None:
                    course = Course(course_link.text, course_link_href, self.author())
                    course.thumbnail = thumbnails.get(course_link_href, None)
                    for chapter_title in course_body.find_all("a"):
                        chapter_href = chapter_title.attrs.get("href", "")
                        if nb_chapter > 2:
                            break
                        print("CHAPTER", nb_chapter, chapter_href)
                        nb_chapter += 1
                        chapter = Chapter(
                            chapter_title.text, chapter_href
                        )
                        chapter.to_file(chapter_basepath)
                        node = chapter.to_node()
                        course.add_node(node)
                    self.add_node(course.to_node())
                else:
                    if course_link.text.strip() == "Agenda":
                        agenda = AgendaOrFlatPage(course_link.text, course_link_href)
                        agenda.to_file(chapter_basepath)
                        self.add_node(agenda.to_node())
                    elif course_link.text.strip() in [
                        "CalcPlot3D Interactive Figures",
                        "GeoGebra Simulations",
                    ]:  # Not supported
                        pass
                    else:
                        course_index = CourseIndex(
                            course_link.text,
                            course_link_href,
                            visited_urls=self.visited_urls,
                        )
                        result = course_index.index(
                            build_path([base_path, hashed(course_link.text)])
                        )
                        if result is None:
                            course_index_node = course_index.to_node()
                            if len(course_index_node["children"]) == 0:
                                chapter = Chapter(course_link.text, course_link_href)
                                chapter.to_file(chapter_basepath)
                                node = chapter.to_node()
                                self.add_node(node)
                            else:
                                self.add_node(course_index_node)
            # break

    def add_node(self, node):
        if node is not None:
            self.tree_nodes[node["source_id"]] = node

    def to_node(self):
        return dict(
            kind=content_kinds.TOPIC,
            source_id=self.source_id,
            title=self.title,
            description=self.description,
            language=self.lang,
            thumbnail=self.thumbnail,
            author=self.author(),
            license=LICENSE,
            children=list(self.tree_nodes.values()),
        )


class Course(object):
    def __init__(self, title, url, author):
        self.source_id = url
        self.title = title
        self.author = author
        self.lang = "en"
        self._thumbnail = None
        self.tree_nodes = OrderedDict()
        LOGGER.info("------- Course: " + self.title)
        LOGGER.info("-------   url: " + self.title)

    @property
    def thumbnail(self):
        return self._thumbnail

    @thumbnail.setter
    def thumbnail(self, url):
        self._thumbnail = save_thumbnail(url, self.title)

    def add_node(self, node):
        self.tree_nodes[node["source_id"]] = node

    def to_node(self):
        return dict(
            kind=content_kinds.TOPIC,
            source_id=self.source_id,
            title=self.title,
            description="",
            language=self.lang,
            thumbnail=self.thumbnail,
            author=self.author,
            license=LICENSE,
            children=list(self.tree_nodes.values()),
        )


class AgendaOrFlatPage(object):
    def __init__(self, title, url):
        self.source_id = url
        self.title = title.replace("/", "_")
        self.soup = self.to_soup()
        self.lang = "en"
        self.filepath = None
        LOGGER.info("--- Agenda (Flat Page)" + self.title)
        LOGGER.info("---   url" + self.source_id)

    def write_css_js(self, filepath):
        with html_writer.HTMLWriter(filepath, "a") as zipper, open(
            "chefdata/styles.css"
        ) as f:
            content = f.read()
            zipper.write_contents("styles.css", content, directory="css/")

        with html_writer.HTMLWriter(filepath, "a") as zipper, open(
            "chefdata/scripts.js"
        ) as f:
            content = f.read()
            zipper.write_contents("scripts.js", content, directory="js/")

    def body(self):
        if self.soup is not None:
            return self.soup.find("section", class_="mt-content-container")

    def clean(self, content):
        link_to_text(content)
        remove_links(content)
        remove_iframes(content)
        remove_scripts(content)
        return content

    def to_soup(self):
        document = download(self.source_id)
        if document is not None:
            return BeautifulSoup(document, "html.parser")

    def write_index(self, filepath, content):
        with html_writer.HTMLWriter(filepath, "w") as zipper:
            zipper.write_index_contents(content)

    def to_file(self, base_path):
        filepath = "{path}/{name}.zip".format(path=base_path, name=hashed(self.title))
        if file_exists(filepath) and OVERWRITE is False:
            self.filepath = filepath
            LOGGER.info("Not overwrited file {}".format(self.filepath))
        elif self.body() is not None:
            self.filepath = filepath
            body = self.clean(self.body())
            try:
                string_to_write = '<html><head><meta charset="utf-8"><link rel="stylesheet" href="css/styles.css"></head><body><div class="main-content-with-sidebar">{}</div><script src="js/scripts.js"></body></html>'.format(
                    body
                )
                self.write_index(
                    self.filepath,
                    string_to_write.encode("utf-8", errors="surrogatepass"),
                )
            except RuntimeError as e:
                self.filepath = None
                LOGGER.error(e)
            else:
                self.write_css_js(self.filepath)
        else:
            LOGGER.error("Empty body in {}".format(self.source_id))

    def to_node(self):
        if self.filepath is not None:
            return dict(
                kind=content_kinds.HTML5,
                source_id=self.source_id,
                title=self.title,
                description="",
                thumbnail=None,
                author="",
                files=[dict(file_type=content_kinds.HTML5, path=self.filepath)],
                language=self.lang,
                license=LICENSE,
            )


class Chapter(AgendaOrFlatPage):
    def __init__(self, title, url):
        self.title = title.replace("/", "_")
        self.source_id = url
        self.soup = self.to_soup()
        self.lang = "en"
        self.filepath = None
        self.video_nodes = None
        self.pdf_nodes = None
        self.phet_nodes = None
        self.author = self.get_author()
        LOGGER.info("--------- Chapter: " + self.title)
        LOGGER.info("---------   url: " + self.source_id)

    def get_author(self):
        if self.soup is not None:
            tag_a = self.soup.find(
                lambda tag: tag.name == "a"
                and tag.findParent("li", "mt-author-information")
            )
            if tag_a is not None:
                return tag_a.text

    def mathjax(self):
        if self.soup is not None:
            scripts = self.soup.find_all("script", type="text/x-mathjax-config")
            return "".join([str(s) for s in scripts])

    def mathjax_dependences(self, filepath):
        mathajax_path = "../MathJax/"
        dependences = [
            "config/TeX-AMS_HTML.js",
            "jax/input/TeX/config.js",
            "jax/input/MathML/config.js",
            "jax/output/SVG/config.js",
            "extensions/tex2jax.js",
            "extensions/mml2jax.js",
            "extensions/MathMenu.js",
            "extensions/MathZoom.js",
            "extensions/TeX/autobold.js",
            "extensions/TeX/mhchem.js",
            "extensions/TeX/color.js",
            "extensions/TeX/boldsymbol.js",
            "extensions/TeX/cancel.js",
            "jax/output/HTML-CSS/jax.js",
            "jax/output/HTML-CSS/fonts/TeX/fontdata.js",
            "jax/output/HTML-CSS/autoload/mtable.js",
            # "jax/output/HTML-CSS/imageFonts.js"
        ]
        for dep in dependences:
            filename = dep.split("/")[-1]
            dep_path = "/".join(dep.split("/")[:-1])
            dep_file_path = os.path.join(mathajax_path, dep_path, filename)
            with html_writer.HTMLWriter(filepath, "a") as zipper, open(
                dep_file_path
            ) as f:
                content = f.read()
                zipper.write_contents(filename, content, directory="js/" + dep_path)

    def to_local_images(self, content):
        images_urls = {}
        for img in content.findAll("img"):
            try:
                img_src = img["src"]
            except KeyError:
                continue
            else:
                if img_src.startswith("/"):
                    img_src = urljoin(BASE_URL, img_src)
                filename = get_name_from_url(img_src)
                if img_src not in images_urls and img_src:
                    img["src"] = filename
                    images_urls[img_src] = filename
        return images_urls

    def build_video_nodes(self, base_path, content):
        videos_url = self.get_videos_urls(content)
        base_path = build_path([DATA_DIR, DATA_DIR_SUBJECT, "videos"])
        video_nodes = []
        for video_url in videos_url:
            if YouTubeResource.is_youtube(video_url) and not YouTubeResource.is_channel(
                video_url
            ):
                video = YouTubeResource(video_url, lang=self.lang)
                video.download(download=DOWNLOAD_VIDEOS, base_path=base_path)
                node = video.to_node()
                if node is not None:
                    video_nodes.append(node)
        return video_nodes

    def build_phet_nodes(self, base_path, content):
        phet_urls = self.get_phet_simulations(content)
        base_path = build_path([DATA_DIR, DATA_DIR_SUBJECT, "phet"])
        phet_nodes = []
        for phet_url in phet_urls:
            phet = PhetResource(self.title, phet_url, lang=self.lang)
            phet.description = None
            phet.download(download=True, base_path=base_path)
            node = phet.to_node()
            if node is not None:
                phet_nodes.append(node)
        return phet_nodes

    def get_videos_urls(self, content):
        urls = set([])
        if content is not None:
            video_urls = content.find_all(
                lambda tag: tag.name == "a"
                and tag.attrs.get("href", "").find("youtube") != -1
                or tag.attrs.get("href", "").find("youtu.be") != -1
                or tag.text.lower() == "youtube"
            )

            for video_url in video_urls:
                urls.add(video_url.get("href", ""))

            for iframe in content.find_all("iframe"):
                url = iframe["src"]
                if YouTubeResource.is_youtube(url) and not YouTubeResource.is_channel(
                    url
                ):
                    urls.add(YouTubeResource.transform_embed(url))
        return urls

    def get_pdfs_urls(self, content):
        urls = set([])
        if content is not None:
            pdf_urls = content.findAll(
                lambda tag: tag.name == "a"
                and tag.attrs.get("href", "").endswith(".pdf")
            )
            for pdf_url in pdf_urls:
                urls.add(pdf_url.get("href", ""))
        return urls

    def get_phet_simulations(self, content):
        urls = set([])
        if content is not None:
            phet_urls = content.find_all(
                lambda tag: tag.name == "iframe"
                and tag.attrs.get("src", "").find("phet.colorado.edu") != -1
            )
            for phet_url in phet_urls:
                urls.add(phet_url.get("src", ""))
        return urls

    def write_images(self, filepath, images):
        with html_writer.HTMLWriter(filepath, "a") as zipper:
            for img_src, img_filename in images.items():
                try:
                    if img_src.startswith("data:image/") or img_src.startswith(
                        "file://"
                    ):
                        pass
                    else:
                        # zipper.write_url(img_src, img_filename, directory="")
                        zipper.write_contents(
                            img_filename,
                            downloader.read(img_src, timeout=5, session=sess),
                            directory="",
                        )
                except (
                    requests.exceptions.HTTPError,
                    requests.exceptions.ConnectTimeout,
                    requests.exceptions.ConnectionError,
                    FileNotFoundError,
                    requests.exceptions.ReadTimeout,
                ):
                    pass

    def build_pdfs_nodes(self, base_path, content):
        pdfs_urls = self.get_pdfs_urls(content)
        base_path = build_path([base_path, "pdfs"])
        pdf_nodes = []
        for pdf_url in pdfs_urls:
            pdf_file = File(pdf_url, lang=self.lang, name=self.title)
            pdf_file.download(download=DOWNLOAD_FILES, base_path=base_path)
            node = pdf_file.to_node()
            if node is not None:
                pdf_nodes.append(node)
        return pdf_nodes

    def write_mathjax(self, filepath):
        script_tag = self.soup.find(
            lambda tag: tag.name == "script"
            and tag.attrs.get("src", "").find("MathJax.js") != -1
        )
        filepath_js = "chefdata/MathJax.js"
        if not file_exists(filepath_js) and script_tag:
            try:
                r = requests.get(script_tag["src"])
                with open(filepath_js, "wb") as f:
                    f.write(r.content)
            except KeyError:
                pass

        with html_writer.HTMLWriter(filepath, "a") as zipper, open(filepath_js) as f:
            content = f.read()
            zipper.write_contents("MathJax.js", content, directory="js/")

    def to_file(self, base_path):
        filepath = "{path}/{name}.zip".format(path=base_path, name=hashed(self.title))
        if self.body() is not None:
            self.video_nodes = self.build_video_nodes(base_path, self.body())
            self.pdf_nodes = self.build_pdfs_nodes(base_path, self.body())
            self.phet_nodes = self.build_phet_nodes(base_path, self.body())
        else:
            LOGGER.error("Empty body in {}".format(self.source_id))
            return

        if file_exists(filepath) and OVERWRITE is False:
            self.filepath = filepath
            LOGGER.info("Not overwrited file {}".format(self.filepath))
        else:
            self.filepath = filepath
            mathjax_scripts = self.mathjax()
            body = self.clean(self.body())
            images = self.to_local_images(body)
            try:
                string_to_write = '<html><head><meta charset="utf-8"><link rel="stylesheet" href="css/styles.css"></head><body><div class="main-content-with-sidebar">{}</div><script src="js/scripts.js"></script>{}<script src="js/MathJax.js?config=TeX-AMS_HTML"></script></body></html>'.format(
                    body, mathjax_scripts
                )
                self.write_index(
                    self.filepath,
                    string_to_write.encode("utf-8", errors="surrogatepass"),
                )
            except RuntimeError as e:
                self.filepath = None
                LOGGER.error(e)
            else:
                self.write_images(self.filepath, images)
                self.write_css_js(self.filepath)
                self.write_mathjax(self.filepath)
                self.mathjax_dependences(self.filepath)

    def topic_node(self):
        return dict(
            kind=content_kinds.TOPIC,
            source_id=self.source_id,
            title=self.title,
            description="",
            language=self.lang,
            author="",
            license=LICENSE,
            thumbnail=None,
            children=[],
        )

    def html_node(self):
        if self.filepath is not None:
            return dict(
                kind=content_kinds.HTML5,
                source_id=self.source_id,
                title=self.title,
                description="",
                thumbnail=None,
                author=self.author,
                files=[dict(file_type=content_kinds.HTML5, path=self.filepath)],
                language=self.lang,
                license=LICENSE,
            )

    def add_to_node(self, node, nodes):
        for node_ in nodes:
            if node_ is not None:
                node["children"].append(node_)

    def to_node(self):
        if self.phet_nodes is not None and len(self.phet_nodes) > 0:
            if len(self.phet_nodes) > 1:
                node = self.topic_node()
                self.add_to_node(node, self.phet_nodes)
            else:
                node = self.phet_nodes[0]
        elif (
            self.video_nodes is not None
            and len(self.video_nodes) > 0
            or self.pdf_nodes is not None
            and len(self.pdf_nodes) > 0
        ):
            node = self.topic_node()
            node["children"].append(self.html_node())
            self.add_to_node(node, self.video_nodes)
            self.add_to_node(node, self.pdf_nodes)
        else:
            node = self.html_node()
        return node


class QueryPage:
    def __init__(self, soup, source_id):
        self.soup = soup
        self.get_id()
        self.source_id = source_id

    def get_id(self):
        page_global_settings = self.soup.find("script", id="mt-global-settings")
        if page_global_settings:
            self.x_deki_token = json.loads(page_global_settings.text).get(
                "apiToken", None
            )
        else:
            self.x_deki_token = None

        query_param = self.soup.find("div", class_="mt-guide-tabs-container")
        if query_param is not None:
            self.page_id = query_param.attrs.get("data-page-id", "")
            query_param = self.soup.find("li", class_="mt-guide-tab")
            self.guid = query_param.attrs.get("data-guid", "")
        else:
            self.page_id = None
            self.guid = None

    def body(self):
        if self.page_id is not None and self.guid is not None:
            url = "{}@api/deki/pages/=Template%253AMindTouch%252FIDF3%252FViews%252FTopic_hierarchy/contents?dream.out.format=json&origin=mt-web&pageid={}&draft=false&guid={}".format(
                BASE_URL, self.page_id, self.guid
            )
            try:
                r = requests.get(
                    url,
                    headers={
                        "x-deki-token": "{}".format(self.x_deki_token),
                        "x-deki-client": "mindtouch-martian",
                        "x-deki-requested-with": "XMLHttpRequest",
                    },
                )
                json_obj = r.json()
                body = json_obj.get("body", None)
                if body is not None:
                    return BeautifulSoup(body, "html.parser")
            except Exception as e:
                LOGGER.error(e)
                return None


class YouTubeResource(object):
    def __init__(
        self,
        source_id,
        name=None,
        type_name="Youtube",
        lang="en",
        embeded=False,
        section_title=None,
        description=None,
    ):
        LOGGER.info("     + Resource Type: {}".format(type_name))
        LOGGER.info("     - URL: {}".format(source_id))
        self.filename = None
        self.type_name = type_name
        self.filepath = None
        if embeded is True:
            self.source_id = YouTubeResource.transform_embed(source_id)
        else:
            self.source_id = self.clean_url(source_id)

        self.name = name
        self.section_title = self.get_name(section_title)
        self.description = description
        self.file_format = file_formats.MP4
        self.lang = lang
        self.is_valid = False

    def clean_url(self, url):
        if url[-1] == "/":
            url = url[:-1]
        return url.strip()

    def get_name(self, name):
        if name is None:
            name = self.source_id.split("/")[-1]
            name = name.split("?")[0]
            return " ".join(name.split("_")).title()
        else:
            return name

    @classmethod
    def is_youtube(self, url, get_channel=False):
        youtube = url.find("youtube") != -1 or url.find("youtu.be") != -1
        if get_channel is False:
            youtube = youtube and url.find("user") == -1 and url.find("/c/") == -1
        return youtube

    @classmethod
    def transform_embed(self, url):
        url = "".join(url.split("?")[:1])
        return url.replace("embed/", "watch?v=").strip()

    @staticmethod
    def is_channel(url):
        return "channel" in url

    def get_video_info(self, download_to=None, subtitles=True):
        ydl_options = {
            "writesubtitles": subtitles,
            "allsubtitles": subtitles,
            "no_warnings": True,
            "restrictfilenames": True,
            "continuedl": True,
            "quiet": False,
            "format": "bestvideo[height<={maxheight}][ext=mp4]+bestaudio[ext=m4a]/best[height<={maxheight}][ext=mp4]".format(
                maxheight="480"
            ),
            "outtmpl": "{}/%(id)s".format(download_to),
            "noplaylist": True,
        }

        with youtube_dl.YoutubeDL(ydl_options) as ydl:
            try:
                ydl.add_default_info_extractors()
                info = ydl.extract_info(
                    self.source_id, download=(download_to is not None)
                )
                return info
            except (
                youtube_dl.utils.DownloadError,
                youtube_dl.utils.ContentTooShortError,
                youtube_dl.utils.ExtractorError,
            ) as e:
                LOGGER.info("An error occured " + str(e))
                LOGGER.info(self.source_id)
            except KeyError as e:
                LOGGER.info(str(e))

    def subtitles_dict(self):
        subs = []
        video_info = self.get_video_info()
        if video_info is not None:
            video_id = video_info.get("id", None)
            if "subtitles" in video_info and video_id is not None:
                subtitles_info = video_info["subtitles"]
                LOGGER.info("Subtitles: {}".format(",".join(subtitles_info.keys())))
                for language in subtitles_info.keys():
                    subs.append(
                        dict(
                            file_type=SUBTITLES_FILE,
                            youtube_id=video_id,
                            language=language,
                        )
                    )
        return subs

    def download(self, download=True, base_path=None):
        download_to = build_path([base_path])
        for i in range(2):
            try:
                info = self.get_video_info(download_to=download_to, subtitles=False)
                if info is not None:
                    LOGGER.info(
                        "    + Video resolution: {}x{}".format(
                            info.get("width", ""), info.get("height", "")
                        )
                    )
                    if self.description is None:
                        self.description = info.get("description", None)
                    self.filepath = os.path.join(
                        download_to, "{}.mp4".format(info["id"])
                    )
                    self.filename = info["title"]
                    if (
                        self.filepath is not None
                        and os.stat(self.filepath).st_size == 0
                    ):
                        LOGGER.info("    + Empty file")
                        self.filepath = None
            except (ValueError, IOError, OSError, URLError, ConnectionResetError) as e:
                LOGGER.info(e)
                LOGGER.info("Download retry")
                time.sleep(0.8)
            except (
                youtube_dl.utils.DownloadError,
                youtube_dl.utils.ContentTooShortError,
                youtube_dl.utils.ExtractorError,
                OSError,
            ) as e:
                LOGGER.info(
                    "    + An error ocurred, may be the video is not available."
                )
                return
            except (OSError, KeyError) as e:
                return
            else:
                return

    def to_node(self):
        if self.filepath is not None:
            files = [dict(file_type=content_kinds.VIDEO, path=self.filepath)]
            files += self.subtitles_dict()
            node = dict(
                kind=content_kinds.VIDEO,
                source_id=self.source_id,
                title=self.name if self.name is not None else self.filename,
                description=self.description,
                author=None,
                files=files,
                language=self.lang,
                license=LICENSE,
            )
            return node


class PhetResource(object):
    def __init__(self, title, url, lang="en"):
        self.source_id = url
        self.title = title
        self.lang = "en"
        self.filepath = None
        self.description = None

    def download(self, download=True, base_path=None):
        # download_to = build_path([base_path])
        dst = tempfile.mkdtemp()
        download_file(
            self.source_id,
            dst,
            filename="index.html",
            request_fn=sess.get,
            middleware_callbacks=[self.process_sim_html],
        )
        self.filepath = create_predictable_zip(dst)

    ##https://github.com/learningequality/sushi-chef-phet/blob/master/chef.py
    def process_sim_html(self, content, destpath, **kwargs):
        """Remove various pieces of the code that make requests to online resources, to avoid using
        bandwidth for users expecting a fully offline or zero-rated website."""

        # remove "are we online" check
        content = content.replace(
            "check:function(){var t=this", "check:function(){return;var t=this"
        )

        # remove online links from "about" modal
        content = content.replace(
            "getLinks:function(", "getLinks:function(){return [];},doNothing:function("
        )

        soup = BeautifulSoup(content, "html.parser")

        for script in soup.find_all("script"):
            # remove Google Analytics and online image bug requests
            if "analytics.js" in str(script):
                script.extract()
            # remove menu options that link to online resources
            if 'createTandem("phetWebsiteButton' in str(script):
                script.string = re.compile(
                    '\{[^}]+createTandem\("phetWebsiteButton"\).*createTandem\("getUpdate"[^\}]*\},'
                ).sub("", script.string.replace("\n", " "))

        return str(soup)

    def to_node(self):
        if self.filepath is not None:
            return dict(
                kind=content_kinds.HTML5,
                source_id=self.source_id,
                title=self.title,
                description=self.description,
                thumbnail=None,
                author="",
                files=[dict(file_type=content_kinds.HTML5, path=self.filepath)],
                language=self.lang,
                license=get_license(
                    licenses.CC_BY,
                    copyright_holder="PhET Interactive Simulations, University of Colorado Boulder",
                ).as_dict(),
            )


class File(object):
    def __init__(self, source_id, lang="en", name=None):
        self.filename = get_name_from_url(source_id)
        self.source_id = (
            urljoin(BASE_URL, source_id) if source_id.startswith("/") else source_id
        )
        self.filepath = None
        self.lang = lang
        self.name = "{}_{}".format(name, self.filename)

    def download(self, download=True, base_path=None):
        try:
            if download is False:
                return
            response = sess.get(self.source_id)
            content_type = response.headers.get("content-type")
            if content_type is not None and "application/pdf" in content_type:
                self.filepath = os.path.join(base_path, self.filename)
                with open(self.filepath, "wb") as f:
                    for chunk in response.iter_content(10000):
                        f.write(chunk)
                LOGGER.info(
                    "    - Get file: {}, node name: {}".format(self.filename, self.name)
                )
        except requests.exceptions.HTTPError as e:
            LOGGER.info("Error: {}".format(e))
        except requests.exceptions.ConnectionError:
            ### this is a weird error, may be it's raised when the webpage
            ### is slow to respond requested resources
            LOGGER.info("Connection error, the resource will be scraped in 5s...")
            time.sleep(3)
        except requests.exceptions.ReadTimeout as e:
            LOGGER.error("Error: {}".format(e))
        except requests.exceptions.TooManyRedirects as e:
            LOGGER.error("Error: {}".format(e))
        except requests.exceptions.InvalidSchema as e:
            LOGGER.error("Error: {}".format(e))

    def to_node(self):
        if self.filepath is not None:
            node = dict(
                kind=content_kinds.DOCUMENT,
                source_id=self.source_id,
                title=self.name,
                description="",
                files=[dict(file_type=content_kinds.DOCUMENT, path=self.filepath)],
                language=self.lang,
                license=LICENSE,
            )
            return node


def download(source_id, loadjs=False):
    tries = 0
    while tries < 20:
        try:
            document = downloader.read(source_id, loadjs=loadjs, session=sess)
        except requests.exceptions.HTTPError as e:
            LOGGER.info("Error: {}".format(e))
        except requests.exceptions.ConnectionError:
            ### this is a weird error, may be it's raised when the webpage
            ### is slow to respond requested resources
            LOGGER.info("Connection error, the resource will be scraped in 5s...")
            time.sleep(5 * tries)
        except requests.exceptions.TooManyRedirects as e:
            LOGGER.info("Error: {}".format(e))
        except (requests.exceptions.InvalidURL, FileNotFoundError) as e:
            LOGGER.error(e)
        else:
            if document is not None:
                return document
        tries += 1
    # return False


def get_index_range(only_pages):
    if only_pages is None:
        from_i = 0
        to_i = None
    else:
        index = only_pages.split(":")
        if len(index) == 2:
            if index[0] == "":
                from_i = 0
                to_i = int(index[1])
            elif index[1] == "":
                from_i = int(index[0])
                to_i = None
            else:
                index = map(int, index)
                from_i, to_i = index
        elif len(index) == 1:
            from_i = int(index[0])
            to_i = from_i + 1
    return from_i, to_i


# The chef subclass
################################################################################
class LibreTextsChef(JsonTreeChef):
    TREES_DATA_DIR = os.path.join(DATA_DIR, "trees")
    SCRAPING_STAGE_OUTPUT_TPL = "ricecooker_{subject}_json_tree.json"
    THUMBNAIL = ""

    def pre_run(self, args, options):
        build_path([LibreTextsChef.TREES_DATA_DIR])
        self.download_css_js()
        channel_tree = self.scrape(args, options)
        self.write_tree_to_json(channel_tree)
        # subject = options.get('--subject', "phys")
        # self.RICECOOKER_JSON_TREE = LibreTextsChef.SCRAPING_STAGE_OUTPUT_TPL.format(subject=subject)

    def download_css_js(self):
        r = requests.get(
            "https://raw.githubusercontent.com/learningequality/html-app-starter/master/css/styles.css"
        )
        with open("chefdata/styles.css", "wb") as f:
            f.write(r.content)

        r = requests.get(
            "https://raw.githubusercontent.com/learningequality/html-app-starter/master/js/scripts.js"
        )
        with open("chefdata/scripts.js", "wb") as f:
            f.write(r.content)

    def scrape(self, args, options):
        only_pages = options.get("--only-pages", None)
        only_videos = options.get("--only-videos", None)
        download_video = options.get("--download-video", "1")
        subject = options.get("--subject", "phys")
        overwrite = options.get("--overwrite", "1")
        run_test = bool(int(options.get("--test", "0")))
        new_channel_id = options.get(
            "--channel-id", None
        )  # can use {subject} as a placeholder
        channel_name = options.get(
            "--channel-name", None
        )  # can use this to set a custom channel name
        channel_description = options.get(
            "--channel-description", None
        )  # can use this to set a custom channel description
        channel_language = options.get(
            "--channel-language", None
        )  # can use this to set channel language

        global DATA_DIR_SUBJECT
        global OVERWRITE
        OVERWRITE = bool(int(overwrite))
        DATA_DIR_SUBJECT = subject
        self.RICECOOKER_JSON_TREE = LibreTextsChef.SCRAPING_STAGE_OUTPUT_TPL.format(
            subject=subject
        )
        self.scrape_stage = os.path.join(
            LibreTextsChef.TREES_DATA_DIR, self.RICECOOKER_JSON_TREE
        )

        LOGGER.info("Scraping {}".format(SUBJECTS[subject]))
        if int(download_video) == 0:
            global DOWNLOAD_VIDEOS
            DOWNLOAD_VIDEOS = False

        global channel_tree
        channel_tree = dict(
            source_domain=SUBJECTS[subject],
            source_id=new_channel_id.format(subject=subject),
            title=channel_name or CHANNEL_NAMES.get(subject, "LibreTexts Channel"),
            description="""Offers a “living library,” curated by students, faculty, and outside experts, of open-source textbooks and curricular materials to support popular secondary and college-level academic subjects, primarily in mathematics and sciences."""[
                :400
            ],
            thumbnail=SUBJECTS_THUMBS[subject],
            author=AUTHOR,
            language=channel_language or "en",
            children=[],
            license=LICENSE,
        )

        global BASE_URL
        BASE_URL = SUBJECTS[subject]

        if run_test is True:
            return test(channel_tree)
        else:
            p_from_i, p_to_i = get_index_range(only_pages)
            v_from_i, v_to_i = get_index_range(only_videos)
            browser = Browser(BASE_URL)
            links = browser.run(p_from_i, p_to_i)
            collections = LinkCollection(links)
            nb_col = 0
            for collection_node in collections.to_node():
                if nb_col > 2:
                    break
                nb_col += 1
                if collection_node is not None:
                    channel_tree["children"].append(collection_node)
            return channel_tree

    def write_tree_to_json(self, channel_tree):
        write_tree_to_json_tree(self.scrape_stage, channel_tree)


def test(channel_tree):
    base_path = build_path([DATA_DIR, DATA_DIR_SUBJECT, hashed("test one: a3")])
    c = Chapter(
        "test: one : one",
        # "https://eng.libretexts.org/Bookshelves/Computer_Science/Book%3A_Eloquent_JavaScript_(Haverbeke)/Part_1%3A_Language/05%3A_Higher-order_Functions",
        "https://eng.libretexts.org/Bookshelves/Materials_Science/TLP_Library_I/03%3A_Atomic_Force_Microscopy/3.07%3A_Scanner_Related_Artefacts",
    )
    c.to_file(base_path)
    channel_tree["children"].append(c.to_node())
    return channel_tree


# CLI
################################################################################
if __name__ == "__main__":
    chef = LibreTextsChef()
    chef.main()
